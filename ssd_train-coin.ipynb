{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD train with ChainerCV with COIN DATASET\n",
    "\n",
    "ChainerCV を使ったSSDの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import chainer\n",
    "from chainer.datasets import ConcatenatedDataset\n",
    "from chainer.datasets import TransformDataset\n",
    "from chainer.optimizer_hooks import WeightDecay\n",
    "from chainer import serializers\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "from chainer.training import triggers\n",
    "\n",
    "#from chainercv.datasets import voc_bbox_label_names\n",
    "#from chainercv.datasets import VOCBboxDataset\n",
    "from lib.voc_bbox_dataset import VOCBboxDataset\n",
    "\n",
    "from chainercv.extensions import DetectionVOCEvaluator\n",
    "from chainercv.links.model.ssd import GradientScaling\n",
    "from chainercv.links.model.ssd import multibox_loss\n",
    "from chainercv.links import SSD300\n",
    "from chainercv.links import SSD512\n",
    "from chainercv import transforms\n",
    "\n",
    "from chainercv.links.model.ssd import random_crop_with_bbox_constraints\n",
    "from chainercv.links.model.ssd import random_distort\n",
    "from chainercv.links.model.ssd import resize_with_random_interpolation\n",
    "\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(img):\n",
    "    plt.imshow(img, interpolation=\"none\")\n",
    "    plt.tick_params(labelbottom='off')\n",
    "    plt.tick_params(labelleft='off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiboxTrainChain(chainer.Chain):\n",
    "\n",
    "    def __init__(self, model, alpha=1, k=3):\n",
    "        super(MultiboxTrainChain, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.model = model\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "\n",
    "    def __call__(self, imgs, gt_mb_locs, gt_mb_labels):\n",
    "        mb_locs, mb_confs = self.model(imgs)\n",
    "        loc_loss, conf_loss = multibox_loss(\n",
    "            mb_locs, mb_confs, gt_mb_locs, gt_mb_labels, self.k)\n",
    "        loss = loc_loss * self.alpha + conf_loss\n",
    "\n",
    "        chainer.reporter.report(\n",
    "            {'loss': loss, 'loss/loc': loc_loss, 'loss/conf': conf_loss},\n",
    "            self)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transform(object):\n",
    "\n",
    "    def __init__(self, coder, size, mean):\n",
    "        # to send cpu, make a copy\n",
    "        self.coder = copy.copy(coder)\n",
    "        self.coder.to_cpu()\n",
    "\n",
    "        self.size = size\n",
    "        self.mean = mean\n",
    "\n",
    "    def __call__(self, in_data):\n",
    "        # There are five data augmentation steps\n",
    "        # 1. Color augmentation\n",
    "        # 2. Random expansion\n",
    "        # 3. Random cropping\n",
    "        # 4. Resizing with random interpolation\n",
    "        # 5. Random horizontal flipping\n",
    "\n",
    "        img, bbox, label = in_data\n",
    "\n",
    "        # 1. Color augmentation\n",
    "        img = random_distort(img)\n",
    "\n",
    "        # 2. Random expansion\n",
    "        if np.random.randint(2):\n",
    "            img, param = transforms.random_expand(\n",
    "                img, fill=self.mean, return_param=True)\n",
    "            bbox = transforms.translate_bbox(\n",
    "                bbox, y_offset=param['y_offset'], x_offset=param['x_offset'])\n",
    "\n",
    "        # 3. Random cropping\n",
    "        img, param = random_crop_with_bbox_constraints(\n",
    "            img, bbox, return_param=True)\n",
    "        bbox, param = transforms.crop_bbox(\n",
    "            bbox, y_slice=param['y_slice'], x_slice=param['x_slice'],\n",
    "            allow_outside_center=False, return_param=True)\n",
    "        label = label[param['index']]\n",
    "\n",
    "        # 4. Resizing with random interpolatation\n",
    "        _, H, W = img.shape\n",
    "        img = resize_with_random_interpolation(img, (self.size, self.size))\n",
    "        bbox = transforms.resize_bbox(bbox, (H, W), (self.size, self.size))\n",
    "\n",
    "        # 5. Random horizontal flipping\n",
    "        img, params = transforms.random_flip(\n",
    "            img, x_random=True, return_param=True)\n",
    "        bbox = transforms.flip_bbox(\n",
    "            bbox, (self.size, self.size), x_flip=params['x_flip'])\n",
    "\n",
    "        # Preparation for SSD network\n",
    "        img -= self.mean\n",
    "        mb_loc, mb_label = self.coder.encode(bbox, label)\n",
    "\n",
    "        return img, mb_loc, mb_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model', choices=('ssd300', 'ssd512'), default='ssd300')\n",
    "parser.add_argument('--batchsize', type=int, default=32)\n",
    "parser.add_argument('--gpu', type=int, default=0)\n",
    "parser.add_argument('--out', default='result')\n",
    "parser.add_argument('--resume')\n",
    "parser.add_argument('-f')\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'result\\\\20180826-205956'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.out = os.path.join(\"result\",datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "args.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.resume = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "硬貨データセット\n",
      "('1', '5', '10', '50', '100', '500')\n"
     ]
    }
   ],
   "source": [
    "DATASET_KIND = \"COIN\"\n",
    "#DATASET_KIND = \"OSATU\"\n",
    "\n",
    "if DATASET_KIND == \"OSATU\":\n",
    "    voc_bbox_label_names = VOCBboxDataset.OSATU_LABEL_NAME\n",
    "    dataset_dir = os.path.join(\".\",\"osatu\")\n",
    "    print (\"お札データセット\")\n",
    "elif DATASET_KIND == \"COIN\":\n",
    "    voc_bbox_label_names = VOCBboxDataset.COIN_LABEL_NAME\n",
    "    dataset_dir = os.path.join(\".\",\"coin\")\n",
    "    print (\"硬貨データセット\")\n",
    "else:\n",
    "    print (\"データセットフォルダを正しく指定して下さい\")\n",
    "print (voc_bbox_label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use model: ssd300\n"
     ]
    }
   ],
   "source": [
    "if args.model == 'ssd300':\n",
    "    model = SSD300(\n",
    "        n_fg_class=len(voc_bbox_label_names),\n",
    "        pretrained_model='imagenet')\n",
    "elif args.model == 'ssd512':\n",
    "    model = SSD512(\n",
    "        n_fg_class=len(voc_bbox_label_names),\n",
    "        pretrained_model='imagenet')\n",
    "print (\"use model: \"+args.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.use_preset('evaluate')\n",
    "train_chain = MultiboxTrainChain(model)\n",
    "if args.gpu >= 0:\n",
    "    chainer.cuda.get_device_from_id(args.gpu).use()\n",
    "    model.to_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = VOCBboxDataset(data_dir=\"jpn_coin_dataset\", split='trainval')\n",
    "train = TransformDataset(\n",
    "    VOCBboxDataset(data_dir=dataset_dir, split='trainval', kind = DATASET_KIND),\n",
    "    Transform(model.coder, model.insize, model.mean))\n",
    "train_iter = chainer.iterators.SerialIterator(train, args.batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = VOCBboxDataset(data_dir=dataset_dir, split='test',use_difficult=True, return_difficult=True, kind = DATASET_KIND)\n",
    "test_iter = chainer.iterators.SerialIterator(test, args.batchsize, repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial lr is set to 1e-3 by ExponentialShift\n",
    "optimizer = chainer.optimizers.MomentumSGD()\n",
    "optimizer.setup(train_chain)\n",
    "for param in train_chain.params():\n",
    "    if param.name == 'b':\n",
    "        param.update_rule.add_hook(GradientScaling(2))\n",
    "    else:\n",
    "        param.update_rule.add_hook(WeightDecay(0.0005))\n",
    "\n",
    "updater = training.updaters.StandardUpdater(\n",
    "    train_iter, optimizer, device=args.gpu)\n",
    "trainer = training.Trainer(updater, (12000, 'iteration'), args.out)\n",
    "trainer.extend(\n",
    "    extensions.ExponentialShift('lr', 0.1, init=1e-3),\n",
    "    trigger=triggers.ManualScheduleTrigger([8000, 10000], 'iteration'))\n",
    "\n",
    "trainer.extend(\n",
    "    DetectionVOCEvaluator(\n",
    "        test_iter, model, use_07_metric=True,\n",
    "        label_names=voc_bbox_label_names),\n",
    "    trigger=(1000, 'iteration'))\n",
    "\n",
    "#log_interval = 100, 'iteration'\n",
    "log_interval = 1, 'epoch'\n",
    "trainer.extend(extensions.LogReport(trigger=log_interval))\n",
    "trainer.extend(extensions.observe_lr(), trigger=log_interval)\n",
    "trainer.extend(extensions.PrintReport(\n",
    "    ['epoch', 'iteration', 'lr',\n",
    "     'main/loss', 'main/loss/loc', 'main/loss/conf',\n",
    "     'validation/main/map']),\n",
    "    trigger=log_interval)\n",
    "#trainer.extend(extensions.ProgressBar(update_interval=10))\n",
    "\n",
    "trainer.extend(extensions.snapshot(), trigger=(2000, 'iteration'))\n",
    "trainer.extend(extensions.snapshot_object(model, 'model_iter_{.updater.iteration}'),trigger=(12000, 'iteration'))\n",
    "\n",
    "if args.resume:\n",
    "    serializers.load_npz(args.resume, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
